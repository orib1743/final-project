import re
import pandas as pd
import numpy as np
import torch
from sklearn.metrics.pairwise import cosine_similarity
from transformers import BertTokenizer, BertModel
from difflib import SequenceMatcher

# ğŸ”¹ ×˜×•×¢×Ÿ ××ª ×”××•×“×œ ×•×”-Tokenizer ×©×œ BERT
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# ×©×™××•×© ×‘-GPU ×× ×§×™×™×
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# ğŸ”¹ ×˜×•×¢×Ÿ ××ª ×§×•×‘×¦×™ ×”××§×¡×œ ×¢× ×”×¤×¡×§××•×ª ×”××—×•×œ×§×•×ª
file_2017_path = r'C:\Users\revit\PycharmProjects\DS_project\Final-Project\Output_Files\Extracted_Hierarchy_Content_2017_Split.xlsx'
file_2018_path = r'C:\Users\revit\PycharmProjects\DS_project\Final-Project\Output_Files\Extracted_Hierarchy_Content_2018_Split.xlsx'

# ×§×¨×™××ª ×”× ×ª×•× ×™×
data_2017 = pd.read_excel(file_2017_path)
data_2018 = pd.read_excel(file_2018_path)

# âœ… ×¤×•× ×§×¦×™×” ×œ×–×™×”×•×™ ×©×™× ×•×™×™ ××¡×¤×•×¨ ×‘×œ×‘×“
def detect_numbering_change(text_2017, text_2018):
    """
    ××–×”×” ×× ×”×©×™× ×•×™ ×”×•× ×¨×§ ×©×™× ×•×™ ××¡×¤×¨ ×¡×¢×™×£, ××š ×œ× ×©×™× ×•×™ ×ª×•×›×Ÿ.
    """
    if pd.isna(text_2017) or pd.isna(text_2018):
        return False

    # ××–×”×” ×¡×™××•× ×™ ×¡×¢×™×¤×™× ×›××• Â§28, Â§45C, Â§30A ×•×›×•'
    num_pattern = r'Â§\d+[A-Z]*'
    numbers_2017 = re.findall(num_pattern, text_2017)
    numbers_2018 = re.findall(num_pattern, text_2018)

    # ×× ×¨×§ ××¡×¤×¨×™ ×”×¡×¢×™×¤×™× ×”×©×ª× ×• ×•×”×ª×•×›×Ÿ ×“×•××” ×‘-90%+, × ×—×©×™×‘ ××ª ×–×” ×›×¨×”-××¨×’×•×Ÿ ×‘×œ×‘×“
    if numbers_2017 != numbers_2018:
        similarity = SequenceMatcher(None, text_2017, text_2018).ratio()
        return similarity > 0.9

    return False

# ğŸ”¹ ×¤×•× ×§×¦×™×” ×œ×—×™×©×•×‘ ×××‘×“×™× ×’×™× ×©×œ BERT
def compute_bert_embedding(text):
    """
    ××§×‘×œ×ª ×˜×§×¡×˜ ×•××—×–×™×¨×” ×•×§×˜×•×¨ ×©×œ 768 ××¡×¤×¨×™× ××ª×•×š BERT
    """
    if pd.isna(text) or len(text.strip()) == 0:
        return np.zeros(768)

    # ×˜×•×§× ×™×–×¦×™×”
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True, padding="max_length")
    inputs = {key: val.to(device) for key, val in inputs.items()}

    # ×”×¢×‘×¨×ª ×”×§×œ×˜ ×“×¨×š ×”××•×“×œ
    with torch.no_grad():
        outputs = model(**inputs)

    # ××—×–×™×¨×™× ××ª ×”-CLS embedding
    return outputs.last_hidden_state[:, 0, :].cpu().numpy().flatten()

# ğŸ”¹ ×—×™×©×•×‘ ×”×××‘×“×™× ×’×™× ×œ×›×œ ×”×¤×¡×§××•×ª ×‘×¢××•×“×ª `split_content`
data_2017["bert_embeddings"] = data_2017["split_content"].apply(compute_bert_embedding)
data_2018["bert_embeddings"] = data_2018["split_content"].apply(compute_bert_embedding)

# ğŸ”¹ ×”×©×•×•××ª ×˜×§×¡×˜×™× ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×§×•×¡×™× ×•×¡×™
similarity_scores = []
numbering_changes = []
for i, row_2017 in data_2017.iterrows():
    if i < len(data_2018):  # × ×•×•×“× ×©×”××™× ×“×§×¡×™× ×ª×•×××™×
        embedding_2017 = row_2017["bert_embeddings"].reshape(1, -1)
        embedding_2018 = data_2018.loc[i, "bert_embeddings"].reshape(1, -1)

        similarity = cosine_similarity(embedding_2017, embedding_2018)[0][0]
        similarity_scores.append(similarity)

        # ×‘×“×™×§×” ×× ××“×•×‘×¨ ×¨×§ ×‘×©×™× ×•×™×™ ××¡×¤×•×¨
        numbering_changes.append(detect_numbering_change(row_2017["split_content"], data_2018.loc[i, "split_content"]))
    else:
        similarity_scores.append(np.nan)
        numbering_changes.append(False)

# ğŸ”¹ ×”×•×¡×¤×ª ×¢××•×“×•×ª ×“××™×•×Ÿ ×•×–×™×”×•×™ ×©×™× ×•×™×™ ××¡×¤×•×¨
data_2017["similarity_2018"] = similarity_scores
data_2017["numbering_change"] = numbering_changes

# ğŸ”¹ ××¦×™××ª ×¤×¡×§××•×ª ×¢× ×©×™× ×•×™ ××©××¢×•×ª×™ (×œ×œ× ×©×™× ×•×™ ××¡×¤×•×¨ ×‘×œ×‘×“)
data_2017["change_detected"] = (data_2017["similarity_2018"] < 0.7) & (~data_2017["numbering_change"])

# âœ… ×©××™×¨×ª ×”×”×©×•×•××”
output_comparison_file = r'C:\Users\revit\PycharmProjects\DS_project\Final-Project\Output_Files\Content_BERT_Comparison.xlsx'
data_2017.to_excel(output_comparison_file, index=False)


print(f"âœ… ×§×•×‘×¥ ×¢× ×”×©×•×•××ª ×”×©×™× ×•×™×™× × ×©××¨: {output_comparison_file}")
